<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Scrapy 爬取伯乐在线实战]]></title>
    <url>%2F2017%2F05%2F16%2FScrapy-%E7%88%AC%E5%8F%96%E4%BC%AF%E4%B9%90%E5%9C%A8%E7%BA%BF%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[爬虫实战 Scrapy爬取伯乐在线 项目介绍 使用Scrapy框架进行爬取伯乐在线的所有技术文章 所用知识点 Scrapy项目的创建 Scrapy框架Shell命令的使用 Scrapy自带的图片下载管道 Scrapy自定义图片下载管道(继承自带的管道) Scrapy框架ItemLoader的使用 Scrapy自定义ItemLoader Scrapy中同步将Item保存入Mysq数据库 Scrapy中异步将Item保存入Mysq数据库 项目初始创建新项目 scrapy startproject bole 创建爬虫 scrapy genspider jobbole blog.jobbole.com 爬虫调试 为了方便对爬虫进行调试，在项目目录中创建一个main.py文件 12345678from scrapy.cmdline import executeimport sys,os # 将项目目录动态设置到环境变量中# os.path.abspath(__file__) 获取main.py的路径# os.path.dirname(os.path.abspath(__file__) 获取main.py所处目录的上一级目录sys.path.append(os.path.dirname(os.path.abspath(__file__)))execute(['scrapy','crawl','jobbole']) 在爬虫开始运行时，建议修改项目中的配置文件,找到123在windows环境下可能会出现```No moudle named &apos;win32api&apos;```,因此需要执行```pip install pypiwin32``` 如果下载速度过慢可使用豆瓣源进行安装```pip install -i https://pypi.douban.com/simple pypiwin32 前置知识XPath语法简介 表达式 说明 article 选取所有article元素的所有子节点 /article 选取根元素article article/a 选取所有属于article的子元素的a元素 article/a/text() 选取所有属于article的子元素的a元素中的文本内容 //div 选取所有div子元素(无论出现在文档的任何地方) //@class 选取所有名为class的属性 /article/div[1] 选取article子元素的第一个div元素 /article/div[last()] 选取article子元素的最后一个div元素 /article/div[last()-1] 选取article子元素的最后第二个div元素 //div[@lang] 选取所有拥有lang属性的div元素 //div[@lang=’eng’] 选取所有lang属性为eng的div元素 /div/* 选取属于div元素下的所有子节点 //* 选取所有元素 //div[@*] 选取所有带有属性的div元素 //div/a \ //div/p 选取所有div元素下的a和p元素 //spand \ //ul 选取文档中的span和ul元素 article/div/p \ //span 选取所有属于article元素的div元素的p元素以及文档中所有span元素 CSS常用选择器 表达式 说明 * 选择所有节点 #container 选择id为container的节点 .container 选择所有class中包含container的节点 li a 选择所有li下面的所有a节点 ul + p 选择ul后面的第一个p元素 div#container &gt; ul 选择id为container的div节点下的第一个ul节点 ul ~ p 选择与ul相邻的所有p元素 a[title] 选择所有包含title属性的a元素 a::text 获得所有a元素的文本内容 a[href=”www.baidu.com”] 选择所有href属性值为www.baidu.com的a元素 a[href*=”baidu”] 选择所有href属性值包含baidu的a元素 a[href^=”www”] 选择所有href属性值以www开头的a元素 a[href$=”.jpg”] 选择所有href属性值以.jpg结尾的a元素 input[type=radio]:checked] 选择所有选中的radio div:not(#container) 选择所有id不为container的div元素 li:nth-child(2) 选择第二个li元素 li:nth-child(2n) 选择第偶数的li元素 Scrapy shell模式 在解析页面的时候如果要查看运行结果则必须要运行Scrapy爬虫发起一个请求，而Scrapy提供了一种方便的调试方法可以只请求一次。shell 网址```12345例如```shellscrpay shell http://blog.jobbole.com/111144/ 文章解析文章详情页Xpath的解析方式1234567891011121314151617181920212223242526272829def parse_detail(self, response): # xpath方式进行解析 # 文章标题 title = response.xpath('//div[@class="entry-header"]/h1/text()').extract_first() # 发布时间 create_time = response.xpath('//p[@class="entry-meta-hide-on-mobile"]/text()').extract_first().replace('·','').strip() # 点赞数 # contains函数是找到class中存在vote-post-up这个类 up_num = response.xpath('//span[contains(@class,"vote-post-up")]/h10/text()').extract_first() # 收藏数 fav_num = response.xpath('//span[contains(@class,"bookmark-btn")]/text()').extract_first() match_re = re.match('.*?(\d+).*',fav_num) if match_re: fav_num = match_re.group(1) else: fav_num = 0 # 评论数 comment_num = response.xpath('//a[@href="#article-comment"]/span/text()').extract_first() match_re = re.match('.*?(\d+).*', comment_num) if match_re: comment_num = match_re.group(1) else: comment_num = 0 # 文章正文 content = response.xpath('//div[@class="entry"]').extract_first() # 获取标签 tags_list = response.xpath('//p[@class="entry-meta-hide-on-mobile"]/a/text()').extract() tags_list = [element for element in tags_list if not element.strip().endswith('评论')] tags = ",".join(tags_list) CSS解析方式 CSS解析的方式使代码更加的简短和明了，因此采用CSS选择器是个较好的选择 12345678910111213141516171819202122232425262728def parse_detail(self, response): # CSS方式进行解析 # 文章标题 title = response.css('div.entry-header h1::text').extract_first() # 发布时间 create_time = response.css('p.entry-meta-hide-on-mobile::text').extract_first().replace('·','').strip() # 点赞数 up_num = response.css('span.vote-post-up h10::text').extract_first() # 收藏数 fav_num = response.css('span.bookmark-btn::text').extract_first() match_re = re.match('.*?(\d+).*',fav_num) if match_re: fav_num = match_re.group(1) else: fav_num = 0 # 评论数 comment_num = response.css('a[href="#article-comment"] span::text').extract_first() match_re = re.match('.*?(\d+).*', comment_num) if match_re: comment_num = match_re.group(1) else: comment_num = 0 # 文章正文 content = response.css('div.entry').extract_first() # 获取标签 tags_list = response.css('p.entry-meta-hide-on-mobile a::text').extract() tags_list = [element for element in tags_list if not element.strip().endswith('评论')] tags = ",".join(tags_list) 列表页 解析完成详情页后，我们要解析列表页获得每一篇文章的url和封面图，然后再交给parse_detail函数去解析 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def parse(self, response): # 获取文章列表中的每一篇文章的url交给Scrapy下载并解析 article_nodes = response.css('div#archive .floated-thumb .post-thumb a') for article_node in article_nodes: # 解析每个文章的封面图 font_image_url = article_node.css('img::attr(src)').extract_first("") # 解析每个文章的url article_url = article_node.css('::attr(href)').extract_first("") # 智能对url进行拼接，如果url中不带有域名则会自动添加域名 # 通过在Request中设置meta信息来进行数据的传递 yield Request(url=parse.urljoin(response.url, article_url),meta=&#123;'font_image_url':parse.urljoin(response.url, font_image_url)&#125;, callback=self.parse_detail) # 获取文章的下一页url地址，并交给自身解析 next_url = response.css('a.next.page-numbers::attr(href)').extract_first('') if next_url: yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse) def parse_detail(self, response): article_item = JobBoleArticleItem() # 从response中获取数据 # 文章封面图 font_image_url = response.meta.get('font_image_url', '') # CSS方式进行解析 # 文章标题 title = response.css('div.entry-header h1::text').extract_first() # 发布时间 create_time = response.css('p.entry-meta-hide-on-mobile::text').extract_first().replace('·','').strip() # 点赞数 up_num = response.css('span.vote-post-up h10::text').extract_first() # 收藏数 fav_num = response.css('span.bookmark-btn::text').extract_first() match_re = re.match('.*?(\d+).*',fav_num) if match_re: fav_num = match_re.group(1) else: fav_num = 0 # 评论数 comment_num = response.css('a[href="#article-comment"] span::text').extract_first() match_re = re.match('.*?(\d+).*', comment_num) if match_re: comment_num = match_re.group(1) else: comment_num = 0 # 文章正文 content = response.css('div.entry').extract_first() # 获取标签 tags_list = response.css('p.entry-meta-hide-on-mobile a::text').extract() tags_list = [element for element in tags_list if not element.strip().endswith('评论')] tags = ",".join(tags_list) article_item["title"] = title article_item["create_time"] = create_time article_item["url"] = response.url article_item["font_image_url"] = [font_image_url] article_item["up_num"] = up_num article_item["fav_num"] = fav_num article_item["comment_num"] = comment_num article_item["content"] = content article_item["tags"] = tags yield article_item 定义Items123456789101112class JobBoleArticleItem(scrapy.Item): title = scrapy.Field() create_time = scrapy.Field() url = scrapy.Field() url_object_id = scrapy.Field() font_image_url = scrapy.Field() font_image_path = scrapy.Field() up_num = scrapy.Field() fav_num = scrapy.Field() comment_num = scrapy.Field() tags = scrapy.Field() content = scrapy.Field() pipeline管道的使用Scrapy自带的图片下载管道 在settings.py中的pipeline处添加 scrapy.pipeline.images.ImagesPipeline 123456789101112ITEM_PIPELINES = &#123; 'bole.pipelines.BolePipeline': 300, 'scrapy.pipeline.images.ImagesPipeline' : 200&#125;# 设置图片url的字段，scraoy将从item中找出此字段进行图片下载IMAGES_URLS_FIELD = "font_image_url"# 设置图片下载保存的目录project_path = os.path.abspath(os.path.dirname(__file__))IMAGES_STORE = os.path.join(project_path, "images")# 表示只下载大于100x100的图片IMAGES_MIN_HEIGHT = 100IMAGES_MIN_WIDTH = 100 之后运行项目可能包PIL未找到，因此需要install pillow```1&gt; 此外scrapy的图片下载默认是接受一个数组，因此在赋值的时候需要```article_item[&quot;font_image_url&quot;] = [font_image_url] 自定义图片下载管道 虽然Scrapy自带的下载中间件很好用，但是如果我要获取图片下载后保存的路径则官方自带就不能满足需求，因此需要我们自定义管道 12345678# 自定义图片下载处理的中间件class ArticleImagePipeline(ImagesPipeline): # 重载函数，改写item处理完成的函数 def item_completed(self, results, item, info): for key, value in results: font_image_path = value["path"] item["font_image_path"] = font_image_path return item 使用Scrapy自带的管道将Item导出成Json文件123456789101112131415161718from scrapy.exporters import JsonItemExporter# 使用Scrapy自带的JsonExporter将item导出为jsonclass JsonExportPipeline(object): # 调用scrapy提供的JsonExporter导出json文件 def __init__(self): self.file = open('article_export.json', 'wb') self.exporter = JsonItemExporter(self.file, encoding="utf-8", ensure_ascii=False) self.exporter.start_exporting() # 重写Item处理 def process_item(self, item, spider): self.exporter.export_item(item) return item def spider_closed(self, spider): self.exporter.finish_exporting() self.file.close() 自定义管道将Item保存为Json文件12345678910111213141516171819202122import codecs,json# 自定义将Item导出为Json的管道class ArticleWithJsonPipeline(object): # 爬虫初始化时调用 def __init__(self): # 打开json文件 # 使用codecs能够解决编码方面的问题 self.file = codecs.open('article.json','w',encoding="utf-8") # 重写Item处理 def process_item(self, item, spider): # 需要关闭ensure_ascii，不然中文字符会显示不正确 lines = json.dump(dict(item), ensure_ascii=False)+'\n' # 将一行数据写入 self.file.write(lines) return item # 爬虫结束时调用 def spider_closed(self, spider): # 关闭文件句柄 self.file.close() 同步化将Item保存入数据库 pip install mysqlclient 安装Mysql客户端库 123456789101112131415161718192021import MySQLdb# 同步机制写入数据库class ArticleWithMysqlPipeline(object): def __init__(self): self.conn = MySQLdb.connect('127.0.0.1', 'root', 'root', 'scrapy', charset="utf8", use_unicode=True) self.cursor = self.conn.cursor() def process_item(self, item, spider): insert_sql = ''' INSERT INTO jobbole_article (title, create_time, url, url_object_id, font_image_url, comment_num, up_num, fav_num, tags, content) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ''' self.cursor.execute(insert_sql, (item["title"], item["create_time"], item["url"], item["url_object_id"], item["font_image_url"][0], item["comment_num"], item["up_num"], item["fav_num"], item["tags"], item["content"])) self.conn.commit() def spider_closed(self, spider): self.conn.close() 异步化将Item保存入数据库 因为Scrapy的解析速度非常快，加上文章的内容较大，因此会出现数据库的操作速度赶不上解析速度会产生阻塞，因此采用异步化的方式来进行数据的插入 12345678910111213141516171819202122232425262728293031323334353637383940414243import MySQLdb.cursorsfrom twisted.enterprise import adbapi# 异步操作写入数据库class ArticleTwiterMysqlPipeline(object): # scrapy会自动执行此方法，将setting文件中的配置读入 @classmethod def from_settings(cls, settings): param = dict( host = settings["MYSQL_HOST"], db = settings["MYSQL_DBNAME"], user = settings["MYSQL_USERNAME"], passwd = settings["MYSQL_PASSWORD"], charset = "utf8", cursorclass = MySQLdb.cursors.DictCursor, use_unicode = True ) #需要使用连接模块的模块名 dbpool = adbapi.ConnectionPool("MySQLdb", **param) return cls(dbpool) def __init__(self, dbpool): self.dbpool = dbpool # 使用twisted异步将数据插入到数据库中 def process_item(self, item, spider): query = self.dbpool.runInteraction(self.do_insert, item) query.addErrback(self.handle_error, item, spider) # 自定义错误处理 def handle_error(self, failure, item, spider): print(failure) print(item) def do_insert(self, cursor, item): insert_sql = ''' INSERT INTO jobbole_article (title, create_time, url, url_object_id, font_image_url, font_image_path, comment_num, up_num, fav_num, tags, content) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ''' cursor.execute(insert_sql, (item["title"], item["create_time"], item["url"], item["url_object_id"], item["font_image_url"][0], item["font_image_path"], item["comment_num"], item["up_num"], item["fav_num"], item["tags"], item["content"])) 项目改进 前面使用了最基本的方式来解析的文章详情页，这样使得spider的代码十分长，不容易维护，因此可以采用自定义ItemLoder的方式方便对规则的管理 spider文件的修改123456789101112131415161718192021222324252627282930313233343536373839404142434445class JobboleSpider(scrapy.Spider): # 爬虫的名称 后续启动爬虫是采用此名称 name = "jobbole" # 爬取允许的域名 allowed_domains = ["blog.jobbole.com"] # 起始url列表 ， 其中的每个URL会进入下面的parse函数进行解析 start_urls = ['http://blog.jobbole.com/all-posts/'] # 列表页面的解析 def parse(self, response): # 获取文章列表中的每一篇文章的url交给Scrapy下载并解析 article_nodes = response.css('div#archive .floated-thumb .post-thumb a') for article_node in article_nodes: # 解析每个文章的封面图 font_image_url = article_node.css('img::attr(src)').extract_first("") # 解析每个文章的url article_url = article_node.css('::attr(href)').extract_first("") # 智能对url进行拼接，如果url中不带有域名则会自动添加域名 # 通过在Request中设置meta信息来进行数据的传递 yield Request(url=parse.urljoin(response.url, article_url),meta=&#123;'font_image_url':parse.urljoin(response.url, font_image_url)&#125;, callback=self.parse_detail) # 获取文章的下一页url地址，并交给自身解析 next_url = response.css('a.next.page-numbers::attr(href)').extract_first('') if next_url: yield Request(url=parse.urljoin(response.url, next_url), callback=self.parse) # 详情页面的解析 def parse_detail(self, response): article_item = JobBoleArticleItem() # 从response中获取文章封面图 font_image_url = response.meta.get('font_image_url', '') item_loader = JobBoleArticleItemLoader(item=JobBoleArticleItem(),response=response) item_loader.add_css('title', 'div.entry-header h1::text') item_loader.add_css('create_time', 'p.entry-meta-hide-on-mobile::text') item_loader.add_value('url', response.url) item_loader.add_value('url_object_id', get_md5(response.url)) item_loader.add_value('font_image_url', [font_image_url]) item_loader.add_css('comment_num', 'a[href="#article-comment"] span::text') item_loader.add_css('content', 'div.entry') item_loader.add_css('tags', 'p.entry-meta-hide-on-mobile a::text') item_loader.add_css('up_num', '.vote-post-up h10') item_loader.add_css('fav_num', 'div.post-adds &gt; span.btn-bluet-bigger.href-style.bookmark-btn.register-user-only::text') article_item = item_loader.load_item() yield article_item 自定义的ItemLoader123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import datetimeimport reimport scrapyfrom scrapy.loader import ItemLoaderfrom scrapy.loader.processors import MapCompose, TakeFirst, Join# 去除文本中的点def remove_dote(value): return value.replace('·','').strip()# 时间转换处理def date_convert(value): try: create_time = datetime.datetime.strptime(value, "%Y/%m/%d").date() except Exception as e: create_time = datetime.datetime.now().date() return create_time# 获得数字def get_num(value): match_re = re.match('.*?(\d+).*', value) if match_re: num = match_re.group(1) else: num = 0 return int(num)# 获取点赞数def get_up_num(value): match_re = re.match('&lt;h10 id=".*?"&gt;(\d+)&lt;/h10&gt;', value) if match_re: num = match_re.group(1) else: num = 0 return int(num)# 去掉tag中的评论def remove_comment_tag(value): if "评论" in value: return "" return value# 默认返回def return_value(value): return value# 自定义ITemLoaderclass JobBoleArticleItemLoader(ItemLoader): # 改写默认的output_processor default_output_processor = TakeFirst()# 伯乐在线Itemclass JobBoleArticleItem(scrapy.Item): title = scrapy.Field() create_time = scrapy.Field( # 该传入的字段值要批量处理的函数 input_processor=MapCompose(remove_dote, date_convert), ) url = scrapy.Field() url_object_id = scrapy.Field() font_image_url = scrapy.Field( output_processor = MapCompose(return_value) ) font_image_path = scrapy.Field() up_num = scrapy.Field( input_processor = MapCompose(get_up_num) ) fav_num = scrapy.Field( input_processor=MapCompose(get_num), ) comment_num = scrapy.Field( input_processor=MapCompose(get_num), ) tags = scrapy.Field( input_processor=MapCompose(remove_comment_tag), output_processor = Join(',') ) content = scrapy.Field()]]></content>
      <tags>
        <tag>爬虫</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyQuery 库学习笔记]]></title>
    <url>%2F2017%2F05%2F15%2FPyQuery-%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[爬虫学习路线第三站 - PyQuery库的使用 初始化字符串初始化12345678910111213141516from pyquery import PyQuery as pyhtml = '''&lt;div&gt; &lt;ul&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="https://ask.hellobi.com/link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="https://ask.hellobi.com/link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="https://ask.hellobi.com/link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''doc = py(html)print(doc('li')) URL初始化12345678from pyquery import PyQuery as py# 通过URL来获取doc = py(url='http://www.baidu.com')# &lt;class 'pyquery.pyquery.PyQuery'&gt;print(type(doc('title')))# 输出选中的head标签print(doc('head')) 文件的初始化12345678from pyquery import PyQuery as py# 通过文件来获取doc = py(filename='demo1.html')# &lt;class 'pyquery.pyquery.PyQuery'&gt;print(type(doc('li')))# 输出所有的li标签print(doc('li')) 基本的CSS选择器1234567891011121314151617from pyquery import PyQuery as pyhtml = '''&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="https://ask.hellobi.com/link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="https://ask.hellobi.com/link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="https://ask.hellobi.com/link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''doc = py(html)# 选中id为container中的class为list中的li标签print(doc('#container .list li')) 查找元素子元素12345678910111213141516171819202122232425262728293031from pyquery import PyQuery as pyhtml = '''&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="https://ask.hellobi.com/link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="https://ask.hellobi.com/link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="https://ask.hellobi.com/link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''doc = py(html)# 获取class为list的元素items = doc('.list')# &lt;class 'pyquery.pyquery.PyQuery'&gt;print(type(items))print(items)# 在先前找到的元素中获取li标签lis = items.find('li')# &lt;class 'pyquery.pyquery.PyQuery'&gt;print(type(lis))print(lis)# 获取先前找到的元素中的所有子元素lis2 = items.children()print(type(lis2))print(lis2)# 获取先前找到的元素中的class为active的元素li3 = items.children('.active')print(li3) 父元素1234567891011121314151617181920212223242526272829from pyquery import PyQuery as pyhtml = '''&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="https://ask.hellobi.com/link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="https://ask.hellobi.com/link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="https://ask.hellobi.com/link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''doc = py(html)# 获取class为list的元素items = doc('.list')# 获取所选元素的父元素container = items.parent()print(type(container))print(container)print("==========================")# 获取所选元素的所有父元素parents = items.parents()print(type(parents))print(parents)print("==========================")# 获取所选元素的所有父元素中class为container的元素parent = items.parents('.container')print(parent) 兄弟元素123456789101112131415161718192021from pyquery import PyQuery as pyhtml = '''&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="https://ask.hellobi.com/link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="https://ask.hellobi.com/link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="https://ask.hellobi.com/link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''doc = py(html)# 获取class为list的元素items = doc('.list')li = doc('.list .item-0.active')# 查找选中元素的所有兄弟元素(不包含自己)print(li.siblings())# 查找选中元素的所有兄弟元素中class为active的元素(不包含自己)print(li.siblings('.active')) 遍历单个元素123456789101112131415161718from pyquery import PyQuery as pyhtml = '''&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="https://ask.hellobi.com/link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="https://ask.hellobi.com/link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="https://ask.hellobi.com/link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''doc = py(html)# 选中单个单个元素li = doc('.item-0.active')print(li) 多个元素123456789101112131415161718192021from pyquery import PyQuery as pyhtml = '''&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="https://ask.hellobi.com/link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="https://ask.hellobi.com/link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="https://ask.hellobi.com/link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''doc = py(html)# 查找所有li标签lis = doc('li').items()# &lt;class 'generator'&gt;print(type(lis))for li in lis: print(li) 获取信息获取属性123456789101112131415161718192021from pyquery import PyQuery as pyhtml = '''&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="https://ask.hellobi.com/link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="https://ask.hellobi.com/link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="https://ask.hellobi.com/link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''doc = py(html)a = doc('.item-0.active a')# &lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;print(a)# link3.html 获取选中标签的href属性print(a.attr('href'))# link3.htmlprint(a.attr.href) 获取文本12345678910111213141516171819from pyquery import PyQuery as pyhtml = '''&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="https://ask.hellobi.com/link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="https://ask.hellobi.com/link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="https://ask.hellobi.com/link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''doc = py(html)a = doc('.item-0.active a')# &lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;print(a)# 获取a标签的内容print(a.text()) 获取HTML12345678910111213141516171819from pyquery import PyQuery as pyhtml = '''&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="https://ask.hellobi.com/link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="https://ask.hellobi.com/link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="https://ask.hellobi.com/link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''doc = py(html)li = doc('.item-1.active')# &lt;li class="item-1 active"&gt;&lt;a href="https://ask.hellobi.com/link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt;print(li)# 获取li标签的HTMLprint(li.html()) DOM操作addClass、removeClass12345678910111213141516171819202122from pyquery import PyQuery as pyhtml = '''&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="https://ask.hellobi.com/link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="https://ask.hellobi.com/link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="https://ask.hellobi.com/link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''doc = py(html)li = doc('.item-0.active')print(li)# 移除classli.removeClass('active')print(li)# 添加classli.addClass('active')print(li) attr、css12345678910111213141516171819202122from pyquery import PyQuery as pyhtml = '''&lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="https://ask.hellobi.com/link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="https://ask.hellobi.com/link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="https://ask.hellobi.com/link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;'''doc = py(html)li = doc('.item-0.active')print(li)# 添加name属性li.attr('name', 'link')print(li)# 添加css样式li.css('font-size', '14px')print(li) remove1234567891011121314from pyquery import PyQuery as pyhtml = '''&lt;div class="wrap"&gt; Hello, World &lt;p&gt;This is a paragraph.&lt;/p&gt; &lt;/div&gt;'''doc = py(html)wrap = doc('.wrap')print(wrap.text())# 在选择的元素中找到p标签并移除wrap.find('p').remove()print(wrap.text()) 其他DOM方法 http://pyquery.readthedocs.io/en/latest/api.html 伪类选择器1234567891011121314151617181920212223242526272829303132333435from pyquery import PyQuery as pyhtml = '''&lt;div class="wrap"&gt; &lt;div id="container"&gt; &lt;ul class="list"&gt; &lt;li class="item-0"&gt;first item&lt;/li&gt; &lt;li class="item-1"&gt;&lt;a href="https://ask.hellobi.com/link2.html"&gt;second item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0 active"&gt;&lt;a href="https://ask.hellobi.com/link3.html"&gt;&lt;span class="bold"&gt;third item&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;li class="item-1 active"&gt;&lt;a href="https://ask.hellobi.com/link4.html"&gt;fourth item&lt;/a&gt;&lt;/li&gt; &lt;li class="item-0"&gt;&lt;a href="https://ask.hellobi.com/link5.html"&gt;fifth item&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt;'''doc = py(html)# 找到第一个lili = doc('li:first-child')print(li)# 找到最后一个lili = doc('li:last-child')print(li)# 找到第二个lili = doc('li:nth-child(2)')print(li)# 找到第三个到最后的lili = doc('li:gt(2)')print(li)# 找到第偶数个lili = doc('li:nth-child(2n)')print(li)# 找到内容包含second的lili = doc('li:contains(second)')print(li) 更多选择器 http://www.w3school.com.cn/css/index.asp 官方文档 http://pyquery.readthedocs.io/]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BeautifulSoup 库学习笔记]]></title>
    <url>%2F2017%2F05%2F15%2FBeautifulSoup-%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[爬虫学习路线第二站 - BeautifulSoup库的使用 常用解析库 BeautifulSoup的基本使用12345678910111213141516171819from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 美化后补全输出print(bs4.prettify())# 输出title标签中的内容print(bs4.title.string) BeautifulSoup标签选择器的用法选择元素123456789101112131415161718192021222324252627from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出title标签 &lt;title&gt;The Dormouse's story&lt;/title&gt;print(bs4.title)# 输出获取到title标签的类型 &lt;class 'bs4.element.Tag'&gt;print(type(bs4.title))# 输出head标签print(bs4.head)# 输出获取到head标签的类型 &lt;class 'bs4.element.Tag'&gt;print(type(bs4.head))# 获取到head标签中的title标签print(bs4.head.title)# 输出p标签(只输出第一个)print(bs4.p) 从上述的代码中可以看出,BeautifulSoup解析出的标签返回任然是一个BeautifulSoup的Tag类，可以再次进行筛选 获取名称1234567891011121314151617from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 获取选择的标签的名称 titleprint(bs4.title.name) 获取属性12345678910111213141516171819from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出p标签的name属性值print(bs4.p['name'])# 输出p标签的name属性值print(bs4.p.attrs['name']) 获取内容12345678910111213141516171819from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出title标签中的内容print(bs4.title.string)# 输出a标签中的内容(去除html标签包括注释)print(bs4.a.string) 嵌套选择1234567891011121314151617from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出head标签中的title标签中的内容print(bs4.head.title.string) 子节点和子孙节点contents12345678910111213141516171819202122html = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 将p标签的子节点以列表的方式输出print(bs4.p.contents) children123456789101112131415161718192021222324252627from bs4 import BeautifulSouphtml = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 获取p标签的所有子节点，返回一个 list 生成器对象print(bs4.p.children)# 对子节点进行遍历for i, child in enumerate(bs4.p.children): print(i, child) descendants123456789101112131415161718192021222324252627from bs4 import BeautifulSouphtml = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 获取p标签的所有子节点(包含子孙节点)，返回一个 list 生成器对象print(bs4.p.descendants)# 对子节点进行遍历for i, child in enumerate(bs4.p.descendants): print(i, child) 父节点和祖先节点parent123456789101112131415161718192021222324from bs4 import BeautifulSouphtml = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出第一个a标签的父节点print(bs4.a.parent) parents12345678910111213141516171819202122232425from bs4 import BeautifulSouphtml = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出循环遍历出所有的祖先节点for i, parent in enumerate(bs4.a.parents): print(i, parent) 兄弟节点1234567891011121314151617181920212223242526from bs4 import BeautifulSouphtml = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出所有前兄弟节点print(list(enumerate(bs4.a.next_siblings)))# 输出所有后兄弟节点print(list(enumerate(bs4.a.previous_siblings))) 标准选择器find_all (返回所有元素) 可根据标签名、属性、内容查找文档 name根据标签名1234567891011121314151617181920212223242526272829from bs4 import BeautifulSouphtml = """&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出所有的ul标签(列表)print(bs4.find_all('ul'))# 输出查找到元素的类型 &lt;class 'bs4.element.Tag'&gt;print(type(bs4.find_all('ul')[0]))for i in bs4.find_all('ul'): # 输出每个ul中的所有li print(i.find_all('li')) attr根据属性123456789101112131415161718192021222324252627282930from bs4 import BeautifulSouphtml = """&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1" name="elements"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出根据id属性查找到的tag元素print(bs4.find_all(attrs=&#123;'id':'list-1'&#125;))# 上述的简写方式print(bs4.find_all(id='list-1'))# 输出根据name属性查找到的tag元素print(bs4.find_all(attrs=&#123;'name':'elements'&#125;))# 根据class查找的话，因为class是python的关键字因此需要加上_print(bs4.find_all(class_='list-small')) text根据文本123456789101112131415161718192021222324from bs4 import BeautifulSouphtml = """&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1" name="elements"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;"""bs4 = BeautifulSoup(html,'lxml')# 通过 text 参数可以搜搜文档中的字符串内容.与 name 参数的可选值一样, text 参数接受 字符串 , 正则表达式 , 列表, Trueprint(bs4.find_all(text='Foo')) find(查找单个)12345678910111213141516171819202122232425262728from bs4 import BeautifulSouphtml = """&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1" name="elements"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出第一个ul标签print(bs4.find('ul'))# &lt;class 'bs4.element.Tag'&gt;print(type(bs4.find('ul')))# 如果找不到则输出Noneprint(bs4.find('page')) 其他用法123456789101112131415161718192021222324252627find_parents() find_parent()find_parents()返回所有祖先节点，find_parent()返回直接父节点。find_next_siblings() find_next_sibling()find_next_siblings()返回后面所有兄弟节点，find_next_sibling()返回后面第一个兄弟节点。find_previous_siblings() find_previous_sibling()find_previous_siblings()返回前面所有兄弟节点，find_previous_sibling()返回前面第一个兄弟节点。find_all_next() find_next()find_all_next()返回节点后所有符合条件的节点, find_next()返回第一个符合条件的节点find_all_previous() 和 find_previous()find_all_previous()返回节点后所有符合条件的节点, find_previous()返回第一个符合条件的节点 CSS选择器 通过select()直接传入CSS选择器即可完成选择 普通选择1234567891011121314151617181920212223242526272829from bs4 import BeautifulSouphtml = """&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;"""bs4 = BeautifulSoup(html,'lxml')print(bs4.select('.panel .panel-heading'))print(bs4.select('ul li'))print(bs4.select('#list-2 .element'))# &lt;class 'list'&gt; 以列表方式输出print(type(bs4.select('ul li')))# &lt;class 'bs4.element.Tag'&gt;print(type(bs4.select('ul')[0])) 获取属性12345678910111213141516171819202122232425262728from bs4 import BeautifulSouphtml = """&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;"""bs4 = BeautifulSoup(html,'lxml')# 查找所有ul并遍历for ul in bs4.select('ul'): # 获取ul的id属性 print(ul['id']) # 获取ul的id属性 print(ul.attrs['id']) 获取内容1234567891011121314151617181920212223242526from bs4 import BeautifulSouphtml = """&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;"""bs4 = BeautifulSoup(html,'lxml')# 查找所有li并遍历for li in bs4.select('li'): # 输出li的文本内容 print(li.get_text())]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Requests 库学习笔记]]></title>
    <url>%2F2017%2F05%2F08%2FRequests-%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[爬虫学习路线第一站 - Requests库的使用 概览实例引入123456789101112131415# 引入Requests库import requests# 发起GET请求response = requests.get('https://www.baidu.com/')# 查看响应类型 requests.models.Responseprint(type(response))# 输出状态码print(response.status_code)# 输出响应内容类型 textprint(type(response.text))# 输出响应内容print(response.text)# 输出cookiesprint(response.cookies) 各种请求方式123456789101112import requests# 发起POST请求requests.post('http://httpbin.org/post')# 发起PUT请求requests.put('http://httpbin.org/put')# 发起DELETE请求requests.delete('http://httpbin.org/delete')# 发送HEAD请求requests.head('http://httpbin.org/get')# 发送OPTION请求requests.options('http://httpbin.org/get') 请求基本GET请求基本写法1234import requestsresponse = requests.get('http://httpbin.org/get')print(response.text) 带参数的GET请求1234import requestsresponse = requests.get('http://httpbin.org/get?name=jyx&amp;age=18')print(response.text) 带参数的GET请求(2)1234567import requests# 分装GET请求参数param = &#123;'name':'jyx','age':19&#125;# 设置GET请求参数(Params)response = requests.get('http://httpbin.org/get',params=param)print(response.text) 解析json123456789import requestsresponse = requests.get('http://httpbin.org/get')# 获取响应内容print(type(response.text))# 如果响应内容是json,就将其转为jsonprint(response.json())# 输出的是字典类型print(type(response.json())) 获取二进制数据1234567891011121314import requestsresponse = requests.get('http://github.com/favicon.ico')# str，bytesprint(type(response.text),type(response.content))# 输出响应的文本内容print(response.text)# 输出响应的二进制内容print(response.content)# 下载二进制数据到本地with open('favicon.ico','wb') as f: f.write(response.content) f.close() 添加headers123456789import requests# 设置User-Agent浏览器信息headers = &#123; "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36"&#125;# 设置请求头信息response = requests.get('https://www.zhihu.com/explore',headers=headers)print(response.text) 基本POST请求1234567891011import requests# 设置传入post表单信息data= &#123; 'name':'jyx', 'age':18&#125;# 设置请求头信息headers = &#123; "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36"&#125;# 设置请求头信息和POST请求参数(data)response = requests.post('http://httpbin.org/post', data=data, headers=headers)print(response.text) 响应response属性12345678910111213import requestsresponse = requests.get('http://www.jianshu.com/')# 获取响应状态码print(type(response.status_code),response.status_code)# 获取响应头信息 print(type(response.headers),response.headers)# 获取响应头中的cookiesprint(type(response.cookies),response.cookies)# 获取访问的url print(type(response.url),response.url)# 获取访问的历史记录 print(type(response.history),response.history) 状态码判断12345678910import requestsresponse = requests.get('http://www.jianshu.com/404.html')# 使用request内置的字母判断状态码if not response.status_code == requests.codes.ok: print('404-1')response = requests.get('http://www.jianshu.com')# 使用状态码数字判断if not response.status_code == 200: print('404-2') requests内置的状态字符1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071100: (&apos;continue&apos;,),101: (&apos;switching_protocols&apos;,),102: (&apos;processing&apos;,),103: (&apos;checkpoint&apos;,),122: (&apos;uri_too_long&apos;, &apos;request_uri_too_long&apos;),200: (&apos;ok&apos;, &apos;okay&apos;, &apos;all_ok&apos;, &apos;all_okay&apos;, &apos;all_good&apos;, &apos;\\o/&apos;, &apos;✓&apos;),201: (&apos;created&apos;,),202: (&apos;accepted&apos;,),203: (&apos;non_authoritative_info&apos;, &apos;non_authoritative_information&apos;),204: (&apos;no_content&apos;,),205: (&apos;reset_content&apos;, &apos;reset&apos;),206: (&apos;partial_content&apos;, &apos;partial&apos;),207: (&apos;multi_status&apos;, &apos;multiple_status&apos;, &apos;multi_stati&apos;, &apos;multiple_stati&apos;),208: (&apos;already_reported&apos;,),226: (&apos;im_used&apos;,),# Redirection.300: (&apos;multiple_choices&apos;,),301: (&apos;moved_permanently&apos;, &apos;moved&apos;, &apos;\\&apos;),302: (&apos;found&apos;,),303: (&apos;see_other&apos;, &apos;other&apos;),304: (&apos;not_modified&apos;,),305: (&apos;use_proxy&apos;,),306: (&apos;switch_proxy&apos;,),307: (&apos;temporary_redirect&apos;, &apos;temporary_moved&apos;, &apos;temporary&apos;),308: (&apos;permanent_redirect&apos;, &apos;resume_incomplete&apos;, &apos;resume&apos;,), # These 2 to be removed in 3.0# Client Error.400: (&apos;bad_request&apos;, &apos;bad&apos;),401: (&apos;unauthorized&apos;,),402: (&apos;payment_required&apos;, &apos;payment&apos;),403: (&apos;forbidden&apos;,),404: (&apos;not_found&apos;, &apos;-&apos;),405: (&apos;method_not_allowed&apos;, &apos;not_allowed&apos;),406: (&apos;not_acceptable&apos;,),407: (&apos;proxy_authentication_required&apos;, &apos;proxy_auth&apos;, &apos;proxy_authentication&apos;),408: (&apos;request_timeout&apos;, &apos;timeout&apos;),409: (&apos;conflict&apos;,),410: (&apos;gone&apos;,),411: (&apos;length_required&apos;,),412: (&apos;precondition_failed&apos;, &apos;precondition&apos;),413: (&apos;request_entity_too_large&apos;,),414: (&apos;request_uri_too_large&apos;,),415: (&apos;unsupported_media_type&apos;, &apos;unsupported_media&apos;, &apos;media_type&apos;),416: (&apos;requested_range_not_satisfiable&apos;, &apos;requested_range&apos;, &apos;range_not_satisfiable&apos;),417: (&apos;expectation_failed&apos;,),418: (&apos;im_a_teapot&apos;, &apos;teapot&apos;, &apos;i_am_a_teapot&apos;),421: (&apos;misdirected_request&apos;,),422: (&apos;unprocessable_entity&apos;, &apos;unprocessable&apos;),423: (&apos;locked&apos;,),424: (&apos;failed_dependency&apos;, &apos;dependency&apos;),425: (&apos;unordered_collection&apos;, &apos;unordered&apos;),426: (&apos;upgrade_required&apos;, &apos;upgrade&apos;),428: (&apos;precondition_required&apos;, &apos;precondition&apos;),429: (&apos;too_many_requests&apos;, &apos;too_many&apos;),431: (&apos;header_fields_too_large&apos;, &apos;fields_too_large&apos;),444: (&apos;no_response&apos;, &apos;none&apos;),449: (&apos;retry_with&apos;, &apos;retry&apos;),450: (&apos;blocked_by_windows_parental_controls&apos;, &apos;parental_controls&apos;),451: (&apos;unavailable_for_legal_reasons&apos;, &apos;legal_reasons&apos;),499: (&apos;client_closed_request&apos;,),# Server Error.500: (&apos;internal_server_error&apos;, &apos;server_error&apos;, &apos;/o\\&apos;, &apos;✗&apos;),501: (&apos;not_implemented&apos;,),502: (&apos;bad_gateway&apos;,),503: (&apos;service_unavailable&apos;, &apos;unavailable&apos;),504: (&apos;gateway_timeout&apos;,),505: (&apos;http_version_not_supported&apos;, &apos;http_version&apos;),506: (&apos;variant_also_negotiates&apos;,),507: (&apos;insufficient_storage&apos;,),509: (&apos;bandwidth_limit_exceeded&apos;, &apos;bandwidth&apos;),510: (&apos;not_extended&apos;,),511: (&apos;network_authentication_required&apos;, &apos;network_auth&apos;, &apos;network_authentication&apos;), 高级操作文件上传123456import requestsfiles = &#123;'file':open('favicon.ico','rb')&#125;# 往POST请求头中设置文件(files)response = requests.post('http://httpbin.org/post',files=files)print(response.text) 获取cookies123456import requestsresponse = requests.get('https://www.baidu.com')print(response.cookies)for key,value in response.cookies.items(): print(key,'=====',value) 会话维持普通请求123456import requestsrequests.get('http://httpbin.org/cookies/set/number/12456')response = requests.get('http://httpbin.org/cookies')# 本质上是两次不同的请求，session不一致print(response.text) 会话维持请求12345678import requests# 从Requests中获取sessionsession = requests.session()# 使用seesion去请求保证了请求是同一个sessionsession.get('http://httpbin.org/cookies/set/number/12456')response = session.get('http://httpbin.org/cookies')print(response.text) 证书验证无证书访问12345import requestsresponse = requests.get('https://www.12306.cn')# 在请求https时，request会进行证书的验证，如果验证失败则会抛出异常print(response.status_code) 关闭证书验证12345import requests# 关闭验证，但是仍然会报出证书警告response = requests.get('https://www.12306.cn',verify=False)print(response.status_code) 消除关闭证书验证的警告1234567from requests.packages import urllib3import requests# 关闭警告urllib3.disable_warnings()response = requests.get('https://www.12306.cn',verify=False)print(response.status_code) 手动设置证书12345import requests# 设置本地证书response = requests.get('https://www.12306.cn', cert=('/path/server.crt', '/path/key'))print(response.status_code) 代理设置设置普通代理12345678910import requestsproxies = &#123; "http": "http://127.0.0.1:9743", "https": "https://127.0.0.1:9743",&#125;# 往请求中设置代理(proxies)response = requests.get("https://www.taobao.com", proxies=proxies)print(response.status_code) 设置带有用户名和密码的代理1234567mport requestsproxies = &#123; "http": "http://user:password@127.0.0.1:9743/",&#125;response = requests.get("https://www.taobao.com", proxies=proxies)print(response.status_code) 设置socks代理 pip3 install ‘requests[socks] 12345678import requestsproxies = &#123; 'http': 'socks5://127.0.0.1:9742', 'https': 'socks5://127.0.0.1:9742'&#125;response = requests.get("https://www.taobao.com", proxies=proxies)print(response.status_code) 超时设置123456789import requestsfrom requests.exceptions import ReadTimeout try: # 设置必须在500ms内收到响应，不然或抛出ReadTimeout异常 response = requests.get("http://httpbin.org/get", timeout=0.5) print(response.status_code)except ReadTimeout: print('Timeout') 认证设置123456import requestsfrom requests.auth import HTTPBasicAuthr = requests.get('http://120.27.34.24:9001', auth=HTTPBasicAuth('user', '123'))# r = requests.get('http://120.27.34.24:9001', auth=('user', '123'))print(r.status_code) 异常处理123456789101112131415import requestsfrom requests.exceptions import ReadTimeout, ConnectionError, RequestExceptiontry: response = requests.get("http://httpbin.org/get", timeout = 0.5) print(response.status_code)except ReadTimeout: # 超时异常 print('Timeout')except ConnectionError: # 连接异常 print('Connection error')except RequestException: # 请求异常 print('Error')]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫库</tag>
      </tags>
  </entry>
</search>