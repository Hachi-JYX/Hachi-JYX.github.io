<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[BeautifulSoup 库学习笔记]]></title>
    <url>%2F2017%2F05%2F15%2FBeautifulSoup-%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[爬虫学习路线第一站 - Requests库的使用 常用解析库 BeautifulSoup的基本使用12345678910111213141516171819from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 美化后补全输出print(bs4.prettify())# 输出title标签中的内容print(bs4.title.string) BeautifulSoup标签选择器的用法选择元素123456789101112131415161718192021222324252627from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出title标签 &lt;title&gt;The Dormouse's story&lt;/title&gt;print(bs4.title)# 输出获取到title标签的类型 &lt;class 'bs4.element.Tag'&gt;print(type(bs4.title))# 输出head标签print(bs4.head)# 输出获取到head标签的类型 &lt;class 'bs4.element.Tag'&gt;print(type(bs4.head))# 获取到head标签中的title标签print(bs4.head.title)# 输出p标签(只输出第一个)print(bs4.p) 从上述的代码中可以看出,BeautifulSoup解析出的标签返回任然是一个BeautifulSoup的Tag类，可以再次进行筛选 获取名称1234567891011121314151617from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 获取选择的标签的名称 titleprint(bs4.title.name) 获取属性12345678910111213141516171819from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出p标签的name属性值print(bs4.p['name'])# 输出p标签的name属性值print(bs4.p.attrs['name']) 获取内容12345678910111213141516171819from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出title标签中的内容print(bs4.title.string)# 输出a标签中的内容(去除html标签包括注释)print(bs4.a.string) 嵌套选择1234567891011121314151617from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出head标签中的title标签中的内容print(bs4.head.title.string) 子节点和子孙节点contents12345678910111213141516171819202122html = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 将p标签的子节点以列表的方式输出print(bs4.p.contents) children123456789101112131415161718192021222324252627from bs4 import BeautifulSouphtml = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 获取p标签的所有子节点，返回一个 list 生成器对象print(bs4.p.children)# 对子节点进行遍历for i, child in enumerate(bs4.p.children): print(i, child) descendants123456789101112131415161718192021222324252627from bs4 import BeautifulSouphtml = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 获取p标签的所有子节点(包含子孙节点)，返回一个 list 生成器对象print(bs4.p.descendants)# 对子节点进行遍历for i, child in enumerate(bs4.p.descendants): print(i, child) 父节点和祖先节点parent123456789101112131415161718192021222324from bs4 import BeautifulSouphtml = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出第一个a标签的父节点print(bs4.a.parent) parents12345678910111213141516171819202122232425from bs4 import BeautifulSouphtml = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出循环遍历出所有的祖先节点for i, parent in enumerate(bs4.a.parents): print(i, parent) 兄弟节点1234567891011121314151617181920212223242526from bs4 import BeautifulSouphtml = """&lt;html&gt; &lt;head&gt; &lt;title&gt;The Dormouse's story&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p class="story"&gt; Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1"&gt; &lt;span&gt;Elsie&lt;/span&gt; &lt;/a&gt; &lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and &lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt; and they lived at the bottom of a well. &lt;/p&gt; &lt;p class="story"&gt;...&lt;/p&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出所有前兄弟节点print(list(enumerate(bs4.a.next_siblings)))# 输出所有后兄弟节点print(list(enumerate(bs4.a.previous_siblings))) 标准选择器find_all (返回所有元素) 可根据标签名、属性、内容查找文档 name根据标签名1234567891011121314151617181920212223242526272829from bs4 import BeautifulSouphtml = """&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出所有的ul标签(列表)print(bs4.find_all('ul'))# 输出查找到元素的类型 &lt;class 'bs4.element.Tag'&gt;print(type(bs4.find_all('ul')[0]))for i in bs4.find_all('ul'): # 输出每个ul中的所有li print(i.find_all('li')) attr根据属性123456789101112131415161718192021222324252627282930from bs4 import BeautifulSouphtml = """&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1" name="elements"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出根据id属性查找到的tag元素print(bs4.find_all(attrs=&#123;'id':'list-1'&#125;))# 上述的简写方式print(bs4.find_all(id='list-1'))# 输出根据name属性查找到的tag元素print(bs4.find_all(attrs=&#123;'name':'elements'&#125;))# 根据class查找的话，因为class是python的关键字因此需要加上_print(bs4.find_all(class_='list-small')) text根据文本123456789101112131415161718192021222324from bs4 import BeautifulSouphtml = """&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1" name="elements"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;"""bs4 = BeautifulSoup(html,'lxml')# 通过 text 参数可以搜搜文档中的字符串内容.与 name 参数的可选值一样, text 参数接受 字符串 , 正则表达式 , 列表, Trueprint(bs4.find_all(text='Foo')) find(查找单个)12345678910111213141516171819202122232425262728from bs4 import BeautifulSouphtml = """&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1" name="elements"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;"""bs4 = BeautifulSoup(html,'lxml')# 输出第一个ul标签print(bs4.find('ul'))# &lt;class 'bs4.element.Tag'&gt;print(type(bs4.find('ul')))# 如果找不到则输出Noneprint(bs4.find('page')) 其他用法123456789101112131415161718192021222324252627find_parents() find_parent()find_parents()返回所有祖先节点，find_parent()返回直接父节点。find_next_siblings() find_next_sibling()find_next_siblings()返回后面所有兄弟节点，find_next_sibling()返回后面第一个兄弟节点。find_previous_siblings() find_previous_sibling()find_previous_siblings()返回前面所有兄弟节点，find_previous_sibling()返回前面第一个兄弟节点。find_all_next() find_next()find_all_next()返回节点后所有符合条件的节点, find_next()返回第一个符合条件的节点find_all_previous() 和 find_previous()find_all_previous()返回节点后所有符合条件的节点, find_previous()返回第一个符合条件的节点 CSS选择器 通过select()直接传入CSS选择器即可完成选择 普通选择1234567891011121314151617181920212223242526272829from bs4 import BeautifulSouphtml = """&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;"""bs4 = BeautifulSoup(html,'lxml')print(bs4.select('.panel .panel-heading'))print(bs4.select('ul li'))print(bs4.select('#list-2 .element'))# &lt;class 'list'&gt; 以列表方式输出print(type(bs4.select('ul li')))# &lt;class 'bs4.element.Tag'&gt;print(type(bs4.select('ul')[0])) 获取属性12345678910111213141516171819202122232425262728from bs4 import BeautifulSouphtml = """&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;"""bs4 = BeautifulSoup(html,'lxml')# 查找所有ul并遍历for ul in bs4.select('ul'): # 获取ul的id属性 print(ul['id']) # 获取ul的id属性 print(ul.attrs['id']) 获取内容1234567891011121314151617181920212223242526from bs4 import BeautifulSouphtml = """&lt;div class="panel"&gt; &lt;div class="panel-heading"&gt; &lt;h4&gt;Hello&lt;/h4&gt; &lt;/div&gt; &lt;div class="panel-body"&gt; &lt;ul class="list" id="list-1"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;li class="element"&gt;Jay&lt;/li&gt; &lt;/ul&gt; &lt;ul class="list list-small" id="list-2"&gt; &lt;li class="element"&gt;Foo&lt;/li&gt; &lt;li class="element"&gt;Bar&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;"""bs4 = BeautifulSoup(html,'lxml')# 查找所有li并遍历for li in bs4.select('li'): # 输出li的文本内容 print(li.get_text())]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Requests 库学习笔记]]></title>
    <url>%2F2017%2F05%2F08%2FRequests-%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[爬虫学习路线第一站 - Requests库的使用 概览实例引入123456789101112131415# 引入Requests库import requests# 发起GET请求response = requests.get('https://www.baidu.com/')# 查看响应类型 requests.models.Responseprint(type(response))# 输出状态码print(response.status_code)# 输出响应内容类型 textprint(type(response.text))# 输出响应内容print(response.text)# 输出cookiesprint(response.cookies) 各种请求方式123456789101112import requests# 发起POST请求requests.post('http://httpbin.org/post')# 发起PUT请求requests.put('http://httpbin.org/put')# 发起DELETE请求requests.delete('http://httpbin.org/delete')# 发送HEAD请求requests.head('http://httpbin.org/get')# 发送OPTION请求requests.options('http://httpbin.org/get') 请求基本GET请求基本写法1234import requestsresponse = requests.get('http://httpbin.org/get')print(response.text) 带参数的GET请求1234import requestsresponse = requests.get('http://httpbin.org/get?name=jyx&amp;age=18')print(response.text) 带参数的GET请求(2)1234567import requests# 分装GET请求参数param = &#123;'name':'jyx','age':19&#125;# 设置GET请求参数(Params)response = requests.get('http://httpbin.org/get',params=param)print(response.text) 解析json123456789import requestsresponse = requests.get('http://httpbin.org/get')# 获取响应内容print(type(response.text))# 如果响应内容是json,就将其转为jsonprint(response.json())# 输出的是字典类型print(type(response.json())) 获取二进制数据1234567891011121314import requestsresponse = requests.get('http://github.com/favicon.ico')# str，bytesprint(type(response.text),type(response.content))# 输出响应的文本内容print(response.text)# 输出响应的二进制内容print(response.content)# 下载二进制数据到本地with open('favicon.ico','wb') as f: f.write(response.content) f.close() 添加headers123456789import requests# 设置User-Agent浏览器信息headers = &#123; "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36"&#125;# 设置请求头信息response = requests.get('https://www.zhihu.com/explore',headers=headers)print(response.text) 基本POST请求1234567891011import requests# 设置传入post表单信息data= &#123; 'name':'jyx', 'age':18&#125;# 设置请求头信息headers = &#123; "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36"&#125;# 设置请求头信息和POST请求参数(data)response = requests.post('http://httpbin.org/post', data=data, headers=headers)print(response.text) 响应response属性12345678910111213import requestsresponse = requests.get('http://www.jianshu.com/')# 获取响应状态码print(type(response.status_code),response.status_code)# 获取响应头信息 print(type(response.headers),response.headers)# 获取响应头中的cookiesprint(type(response.cookies),response.cookies)# 获取访问的url print(type(response.url),response.url)# 获取访问的历史记录 print(type(response.history),response.history) 状态码判断12345678910import requestsresponse = requests.get('http://www.jianshu.com/404.html')# 使用request内置的字母判断状态码if not response.status_code == requests.codes.ok: print('404-1')response = requests.get('http://www.jianshu.com')# 使用状态码数字判断if not response.status_code == 200: print('404-2') requests内置的状态字符1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071100: (&apos;continue&apos;,),101: (&apos;switching_protocols&apos;,),102: (&apos;processing&apos;,),103: (&apos;checkpoint&apos;,),122: (&apos;uri_too_long&apos;, &apos;request_uri_too_long&apos;),200: (&apos;ok&apos;, &apos;okay&apos;, &apos;all_ok&apos;, &apos;all_okay&apos;, &apos;all_good&apos;, &apos;\\o/&apos;, &apos;✓&apos;),201: (&apos;created&apos;,),202: (&apos;accepted&apos;,),203: (&apos;non_authoritative_info&apos;, &apos;non_authoritative_information&apos;),204: (&apos;no_content&apos;,),205: (&apos;reset_content&apos;, &apos;reset&apos;),206: (&apos;partial_content&apos;, &apos;partial&apos;),207: (&apos;multi_status&apos;, &apos;multiple_status&apos;, &apos;multi_stati&apos;, &apos;multiple_stati&apos;),208: (&apos;already_reported&apos;,),226: (&apos;im_used&apos;,),# Redirection.300: (&apos;multiple_choices&apos;,),301: (&apos;moved_permanently&apos;, &apos;moved&apos;, &apos;\\&apos;),302: (&apos;found&apos;,),303: (&apos;see_other&apos;, &apos;other&apos;),304: (&apos;not_modified&apos;,),305: (&apos;use_proxy&apos;,),306: (&apos;switch_proxy&apos;,),307: (&apos;temporary_redirect&apos;, &apos;temporary_moved&apos;, &apos;temporary&apos;),308: (&apos;permanent_redirect&apos;, &apos;resume_incomplete&apos;, &apos;resume&apos;,), # These 2 to be removed in 3.0# Client Error.400: (&apos;bad_request&apos;, &apos;bad&apos;),401: (&apos;unauthorized&apos;,),402: (&apos;payment_required&apos;, &apos;payment&apos;),403: (&apos;forbidden&apos;,),404: (&apos;not_found&apos;, &apos;-&apos;),405: (&apos;method_not_allowed&apos;, &apos;not_allowed&apos;),406: (&apos;not_acceptable&apos;,),407: (&apos;proxy_authentication_required&apos;, &apos;proxy_auth&apos;, &apos;proxy_authentication&apos;),408: (&apos;request_timeout&apos;, &apos;timeout&apos;),409: (&apos;conflict&apos;,),410: (&apos;gone&apos;,),411: (&apos;length_required&apos;,),412: (&apos;precondition_failed&apos;, &apos;precondition&apos;),413: (&apos;request_entity_too_large&apos;,),414: (&apos;request_uri_too_large&apos;,),415: (&apos;unsupported_media_type&apos;, &apos;unsupported_media&apos;, &apos;media_type&apos;),416: (&apos;requested_range_not_satisfiable&apos;, &apos;requested_range&apos;, &apos;range_not_satisfiable&apos;),417: (&apos;expectation_failed&apos;,),418: (&apos;im_a_teapot&apos;, &apos;teapot&apos;, &apos;i_am_a_teapot&apos;),421: (&apos;misdirected_request&apos;,),422: (&apos;unprocessable_entity&apos;, &apos;unprocessable&apos;),423: (&apos;locked&apos;,),424: (&apos;failed_dependency&apos;, &apos;dependency&apos;),425: (&apos;unordered_collection&apos;, &apos;unordered&apos;),426: (&apos;upgrade_required&apos;, &apos;upgrade&apos;),428: (&apos;precondition_required&apos;, &apos;precondition&apos;),429: (&apos;too_many_requests&apos;, &apos;too_many&apos;),431: (&apos;header_fields_too_large&apos;, &apos;fields_too_large&apos;),444: (&apos;no_response&apos;, &apos;none&apos;),449: (&apos;retry_with&apos;, &apos;retry&apos;),450: (&apos;blocked_by_windows_parental_controls&apos;, &apos;parental_controls&apos;),451: (&apos;unavailable_for_legal_reasons&apos;, &apos;legal_reasons&apos;),499: (&apos;client_closed_request&apos;,),# Server Error.500: (&apos;internal_server_error&apos;, &apos;server_error&apos;, &apos;/o\\&apos;, &apos;✗&apos;),501: (&apos;not_implemented&apos;,),502: (&apos;bad_gateway&apos;,),503: (&apos;service_unavailable&apos;, &apos;unavailable&apos;),504: (&apos;gateway_timeout&apos;,),505: (&apos;http_version_not_supported&apos;, &apos;http_version&apos;),506: (&apos;variant_also_negotiates&apos;,),507: (&apos;insufficient_storage&apos;,),509: (&apos;bandwidth_limit_exceeded&apos;, &apos;bandwidth&apos;),510: (&apos;not_extended&apos;,),511: (&apos;network_authentication_required&apos;, &apos;network_auth&apos;, &apos;network_authentication&apos;), 高级操作文件上传123456import requestsfiles = &#123;'file':open('favicon.ico','rb')&#125;# 往POST请求头中设置文件(files)response = requests.post('http://httpbin.org/post',files=files)print(response.text) 获取cookies123456import requestsresponse = requests.get('https://www.baidu.com')print(response.cookies)for key,value in response.cookies.items(): print(key,'=====',value) 会话维持普通请求123456import requestsrequests.get('http://httpbin.org/cookies/set/number/12456')response = requests.get('http://httpbin.org/cookies')# 本质上是两次不同的请求，session不一致print(response.text) 会话维持请求12345678import requests# 从Requests中获取sessionsession = requests.session()# 使用seesion去请求保证了请求是同一个sessionsession.get('http://httpbin.org/cookies/set/number/12456')response = session.get('http://httpbin.org/cookies')print(response.text) 证书验证无证书访问12345import requestsresponse = requests.get('https://www.12306.cn')# 在请求https时，request会进行证书的验证，如果验证失败则会抛出异常print(response.status_code) 关闭证书验证12345import requests# 关闭验证，但是仍然会报出证书警告response = requests.get('https://www.12306.cn',verify=False)print(response.status_code) 消除关闭证书验证的警告1234567from requests.packages import urllib3import requests# 关闭警告urllib3.disable_warnings()response = requests.get('https://www.12306.cn',verify=False)print(response.status_code) 手动设置证书12345import requests# 设置本地证书response = requests.get('https://www.12306.cn', cert=('/path/server.crt', '/path/key'))print(response.status_code) 代理设置设置普通代理12345678910import requestsproxies = &#123; "http": "http://127.0.0.1:9743", "https": "https://127.0.0.1:9743",&#125;# 往请求中设置代理(proxies)response = requests.get("https://www.taobao.com", proxies=proxies)print(response.status_code) 设置带有用户名和密码的代理1234567mport requestsproxies = &#123; "http": "http://user:password@127.0.0.1:9743/",&#125;response = requests.get("https://www.taobao.com", proxies=proxies)print(response.status_code) 设置socks代理 pip3 install ‘requests[socks] 12345678import requestsproxies = &#123; 'http': 'socks5://127.0.0.1:9742', 'https': 'socks5://127.0.0.1:9742'&#125;response = requests.get("https://www.taobao.com", proxies=proxies)print(response.status_code) 超时设置123456789import requestsfrom requests.exceptions import ReadTimeout try: # 设置必须在500ms内收到响应，不然或抛出ReadTimeout异常 response = requests.get("http://httpbin.org/get", timeout=0.5) print(response.status_code)except ReadTimeout: print('Timeout') 认证设置123456import requestsfrom requests.auth import HTTPBasicAuthr = requests.get('http://120.27.34.24:9001', auth=HTTPBasicAuth('user', '123'))# r = requests.get('http://120.27.34.24:9001', auth=('user', '123'))print(r.status_code) 异常处理123456789101112131415import requestsfrom requests.exceptions import ReadTimeout, ConnectionError, RequestExceptiontry: response = requests.get("http://httpbin.org/get", timeout = 0.5) print(response.status_code)except ReadTimeout: # 超时异常 print('Timeout')except ConnectionError: # 连接异常 print('Connection error')except RequestException: # 请求异常 print('Error')]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫库</tag>
      </tags>
  </entry>
</search>